import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import numpy as np

class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size=10000, d_model=512, nhead=8, num_layers=6, max_seq_len=512):
        super(SimpleTransformer, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=2048,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.output_layer = nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        # åµŒå…¥å±‚ + ä½ç½®ç¼–ç 
        x = self.embedding(x) * np.sqrt(self.d_model)
        x = x + self.pos_encoding[:, :x.size(1), :]
        
        # åˆ›å»ºæ³¨æ„åŠ›mask
        seq_len = x.size(1)
        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)
        
        # Transformerç¼–ç å™¨
        x = self.transformer(x, mask=mask)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        return x

def check_matmul_device():
    """æ£€æŸ¥torch.matmulç®—å­ä½¿ç”¨çš„è®¾å¤‡å¹¶æ¯”è¾ƒæ€§èƒ½"""
    print("æ£€æŸ¥torch.matmulç®—å­è®¾å¤‡...")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡ - ä½¿ç”¨æ›´å¤§çš„çŸ©é˜µä»¥æ›´å¥½åœ°ä½“ç°æ€§èƒ½å·®å¼‚
    size = 1024
    a_cpu = torch.randn(size, size)
    b_cpu = torch.randn(size, size)
    
    # CPUä¸Šçš„matmul - è¿è¡Œ100æ¬¡
    print("CPU matmulæµ‹è¯•...")
    start_time = time.time()
    for i in range(100):
        result_cpu = torch.matmul(a_cpu, b_cpu)
    cpu_time = time.time() - start_time
    print(f"CPU matmul 100æ¬¡æ€»æ—¶é—´: {cpu_time:.4f}ç§’")
    print(f"CPU matmulå¹³å‡æ¯æ¬¡æ—¶é—´: {cpu_time/100:.6f}ç§’")
    print(f"CPU matmulç»“æœè®¾å¤‡: {result_cpu.device}")
    
    # æ£€æŸ¥MPSæ˜¯å¦å¯ç”¨
    if torch.backends.mps.is_available():
        a_mps = a_cpu.to('mps')
        b_mps = b_cpu.to('mps')
        
        # MPSä¸Šçš„matmul - è¿è¡Œ100æ¬¡
        print("\nMPS matmulæµ‹è¯•...")
        
        # é¢„çƒ­è¿è¡Œå‡ æ¬¡ä»¥é¿å…ç¬¬ä¸€æ¬¡è¿è¡Œçš„åˆå§‹åŒ–å¼€é”€
        for _ in range(5):
            _ = torch.matmul(a_mps, b_mps)
        
        # æ­£å¼æµ‹è¯•100æ¬¡
        start_time = time.time()
        for i in range(100):
            result_mps = torch.matmul(a_mps, b_mps)
        mps_time = time.time() - start_time
        print(f"MPS matmul 100æ¬¡æ€»æ—¶é—´: {mps_time:.4f}ç§’")
        print(f"MPS matmulå¹³å‡æ¯æ¬¡æ—¶é—´: {mps_time/100:.6f}ç§’")
        print(f"MPS matmulç»“æœè®¾å¤‡: {result_mps.device}")
        
        # æ€§èƒ½æ¯”è¾ƒ
        speedup = cpu_time / mps_time
        print(f"\næ€§èƒ½æ¯”è¾ƒ:")
        print(f"CPUæ€»æ—¶é—´: {cpu_time:.4f}ç§’")
        print(f"MPSæ€»æ—¶é—´: {mps_time:.4f}ç§’")
        print(f"åŠ é€Ÿæ¯” (CPU/MPS): {speedup:.2f}x")
        
        if speedup > 1:
            print(f"ğŸ‰ MPSæ¯”CPUå¿« {speedup:.2f} å€")
        else:
            print(f"âš ï¸ CPUæ¯”MPSå¿« {1/speedup:.2f} å€")
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ç›¸åŒï¼ˆå…è®¸å°çš„æ•°å€¼å·®å¼‚ï¼‰
        result_mps_cpu = result_mps.cpu()
        diff = torch.abs(result_cpu - result_mps_cpu).max()
        print(f"CPUå’ŒMPSç»“æœæœ€å¤§å·®å¼‚: {diff.item():.8f}")
        
        # é¢å¤–æµ‹è¯•ï¼šä¸åŒçŸ©é˜µå¤§å°çš„æ€§èƒ½
        print("\n" + "="*50)
        print("ä¸åŒçŸ©é˜µå¤§å°çš„æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        sizes = [256, 512, 1024, 2048]
        for test_size in sizes:
            print(f"\næµ‹è¯•çŸ©é˜µå¤§å°: {test_size}x{test_size}")
            test_a_cpu = torch.randn(test_size, test_size)
            test_b_cpu = torch.randn(test_size, test_size)
            
            # CPUæµ‹è¯•
            start = time.time()
            for _ in range(20):  # å‡å°‘æ¬¡æ•°ä»¥åŠ å¿«æµ‹è¯•
                _ = torch.matmul(test_a_cpu, test_b_cpu)
            test_cpu_time = time.time() - start
            
            # MPSæµ‹è¯•
            test_a_mps = test_a_cpu.to('mps')
            test_b_mps = test_b_cpu.to('mps')
            
            # é¢„çƒ­
            for _ in range(3):
                _ = torch.matmul(test_a_mps, test_b_mps)
            
            start = time.time()
            for _ in range(20):
                _ = torch.matmul(test_a_mps, test_b_mps)
            test_mps_time = time.time() - start
            
            test_speedup = test_cpu_time / test_mps_time
            print(f"CPU: {test_cpu_time/20:.6f}s/æ¬¡, MPS: {test_mps_time/20:.6f}s/æ¬¡, åŠ é€Ÿ: {test_speedup:.2f}x")
            
    else:
        print("MPSä¸å¯ç”¨")

# åœ¨mainå‡½æ•°ä¸­è°ƒç”¨è¿™ä¸ªå‡½æ•°
if __name__ == "__main__":
    print("PyTorchç‰ˆæœ¬:", torch.__version__)
    print("MPSå¯ç”¨:", torch.backends.mps.is_available())
    
    if torch.backends.mps.is_available():
        print("MPSè®¾å¤‡:", )
    
    check_matmul_device()

def benchmark_model(device='cpu', batch_size=4, seq_len=128):
    """åœ¨æŒ‡å®šè®¾å¤‡ä¸Šæµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    print(f"\nåœ¨ {device.upper()} ä¸Šè¿›è¡Œæ€§èƒ½æµ‹è¯•...")
    
    # åˆ›å»ºæ¨¡å‹å’Œæµ‹è¯•æ•°æ®
    model = SimpleTransformer(
        vocab_size=10000,
        d_model=256,  # å‡å°æ¨¡å‹å¤§å°ä»¥ä¾¿æ›´å¿«æµ‹è¯•
        nhead=8,
        num_layers=4,
        max_seq_len=seq_len
    ).to(device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(device)
    
    # é¢„çƒ­ï¼ˆé¿å…ç¬¬ä¸€æ¬¡è¿è¡Œçš„åˆå§‹åŒ–å¼€é”€ï¼‰
    for _ in range(3):
        _ = model(input_ids)
    
    # æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    num_iterations = 50
    
    for i in range(num_iterations):
        output = model(input_ids)
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
        if i % 10 == 0:  # æ¯10æ¬¡è¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­
            loss = output.mean()
            loss.backward()
    
    end_time = time.time()
    
    total_time = end_time - start_time
    avg_time_per_iteration = total_time / num_iterations
    
    print(f"è®¾å¤‡: {device}")
    print(f"æ€»æ—¶é—´: {total_time:.4f}ç§’")
    print(f"æ¯æ¬¡è¿­ä»£å¹³å‡æ—¶é—´: {avg_time_per_iteration:.4f}ç§’")
    print(f"ååé‡: {num_iterations/total_time:.2f} è¿­ä»£/ç§’")
    
    return total_time, avg_time_per_iteration

def compare_performance():
    """æ¯”è¾ƒCPUå’ŒMPSæ€§èƒ½"""
    print("=" * 50)
    print("CPU vs MPS æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    mps_available = torch.backends.mps.is_available()
    print(f"MPSå¯ç”¨: {mps_available}")
    
    if mps_available:
        print("MPSè®¾å¤‡: ")
    
    # æ£€æŸ¥matmulè®¾å¤‡
    check_matmul_device()
    
    # æµ‹è¯•ä¸åŒæ‰¹å¤§å°å’Œåºåˆ—é•¿åº¦
    test_configs = [
        (4, 128),
        (8, 128),
        (4, 256),
    ]
    
    cpu_times = []
    mps_times = []
    
    for batch_size, seq_len in test_configs:
        print(f"\næµ‹è¯•é…ç½®: batch_size={batch_size}, seq_len={seq_len}")
        print("-" * 40)
        
        # CPUæµ‹è¯•
        cpu_total, cpu_avg = benchmark_model('cpu', batch_size, seq_len)
        cpu_times.append((cpu_total, cpu_avg))
        
        # MPSæµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if mps_available:
            mps_total, mps_avg = benchmark_model('mps', batch_size, seq_len)
            mps_times.append((mps_total, mps_avg))
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = cpu_avg / mps_avg
            print(f"\nåŠ é€Ÿæ¯” (CPU/MPS): {speedup:.2f}x")
            
            if speedup > 1:
                print(f"MPSæ¯”CPUå¿« {speedup:.2f} å€")
            else:
                print(f"CPUæ¯”MPSå¿« {1/speedup:.2f} å€")
    
    # æ€»ç»“
    if mps_available and mps_times:
        print("\n" + "=" * 50)
        print("æ€§èƒ½æ¯”è¾ƒæ€»ç»“")
        print("=" * 50)
        
        avg_cpu_time = np.mean([t[1] for t in cpu_times])
        avg_mps_time = np.mean([t[1] for t in mps_times])
        overall_speedup = avg_cpu_time / avg_mps_time
        
        print(f"å¹³å‡CPUæ—¶é—´: {avg_cpu_time:.4f}ç§’/è¿­ä»£")
        print(f"å¹³å‡MPSæ—¶é—´: {avg_mps_time:.4f}ç§’/è¿­ä»£")
        print(f"æ€»ä½“åŠ é€Ÿæ¯”: {overall_speedup:.2f}x")
        
        if overall_speedup > 1:
            print(f"ğŸ‰ MPSæ€»ä½“è¡¨ç°æ›´å¥½ï¼ŒåŠ é€Ÿ {overall_speedup:.2f} å€")
        else:
            print(f"âš ï¸ CPUæ€»ä½“è¡¨ç°æ›´å¥½ï¼ŒMPSæ…¢ {1/overall_speedup:.2f} å€")

def memory_usage_comparison():
    """æ¯”è¾ƒå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\n" + "=" * 50)
    print("å†…å­˜ä½¿ç”¨æ¯”è¾ƒ")
    print("=" * 50)
    
    if torch.backends.mps.is_available():
        # åœ¨MPSä¸Šæµ‹è¯•å†…å­˜ä½¿ç”¨
        model_mps = SimpleTransformer(d_model=256, num_layers=4).to('mps')
        input_mps = torch.randint(0, 10000, (4, 128)).to('mps')
        
        # MPSå†…å­˜ç»Ÿè®¡
        if hasattr(torch.mps, 'memory_allocated'):
            torch.mps.empty_cache()
            initial_mem = torch.mps.memory_allocated() / 1024**2  # MB
            
            output_mps = model_mps(input_mps)
            peak_mem = torch.mps.memory_allocated() / 1024**2
            
            print(f"MPSå†…å­˜ä½¿ç”¨:")
            print(f"  åˆå§‹: {initial_mem:.2f} MB")
            print(f"  å³°å€¼: {peak_mem:.2f} MB")
            print(f"  å¢åŠ : {peak_mem - initial_mem:.2f} MB")
    
    # CPUå†…å­˜ç»Ÿè®¡
    import psutil
    process = psutil.Process()
    initial_cpu_mem = process.memory_info().rss / 1024**2
    
    model_cpu = SimpleTransformer(d_model=256, num_layers=4)
    input_cpu = torch.randint(0, 10000, (4, 128))
    output_cpu = model_cpu(input_cpu)
    
    final_cpu_mem = process.memory_info().rss / 1024**2
    
    print(f"CPUå†…å­˜ä½¿ç”¨:")
    print(f"  åˆå§‹: {initial_cpu_mem:.2f} MB")
    print(f"  å³°å€¼: {final_cpu_mem:.2f} MB")
    print(f"  å¢åŠ : {final_cpu_mem - initial_cpu_mem:.2f} MB")

if __name__ == "__main__":
    print("PyTorchç‰ˆæœ¬:", torch.__version__)
    print("MPSå¯ç”¨:", torch.backends.mps.is_available())
    
    if torch.backends.mps.is_available():
        print("MPSè®¾å¤‡:", )
    
    # è¿è¡Œæ€§èƒ½æ¯”è¾ƒ
    compare_performance()
    
    # è¿è¡Œå†…å­˜ä½¿ç”¨æ¯”è¾ƒ
    memory_usage_comparison()
    
    print("\næµ‹è¯•å®Œæˆï¼")
